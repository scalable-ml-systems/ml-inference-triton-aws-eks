# scalable-triton-inference-eks 

GPU-enabled Triton inference server on Kubernetes/EKS â€” reproducible, secure, and cost-aware.

This repository provides a production-ready blueprint for serving AI models at scale. 

---

This project delivers a **cloud-native MLOps platform** purpose-built for GPU inference workloads. It combines **NVIDIA Triton Inference Server** with **AWS EKS** to provide: 

-  **High-performance inference** â†’ GPU acceleration for deep learning, LLMs, and computer vision models 
-  **Reproducibility** â†’ Infrastructure-as-code with Terraform and Helm ensures consistent environments across dev, staging, and prod 
-  **Security** â†’ IAM roles via IRSA, scoped permissions, and GitHub OIDC integration for CI/CD trust 
-  **Cost-awareness** â†’ Separate GPU and CPU node groups, teardown hygiene, and lifecycle automation prevent waste 
-  **Observability** â†’ Hooks for Prometheus, Grafana, Loki, and CloudWatch deliver monitoring, logging, and compliance visibility
-  **Scalability & Resilience** â†’ Modular Kubernetes design supports multi-tenant workloads and future service mesh integration

---

## ğŸ’¼ Business Value

This platform accelerates AI/ML adoption across industries:

- ğŸ§¬ **Biotech & Medicine** â†’ Enables faster drug discovery, medical imaging analysis, and precision diagnostics at scale  
- ğŸ’³ **Finance & Insurance** â†’ Powers fraud detection, risk modeling, and realâ€‘time customer insights with secure GPU inference  
- ğŸ¨ **Hospitality & Retail** â†’ Delivers personalized recommendations, demand forecasting, and customer experience optimization  
- ğŸŒ¾ **Agriculture & Energy** â†’ Supports crop yield prediction, resource optimization, and sustainable energy analytics  
- ğŸ­ **Manufacturing & Logistics** â†’ Improves predictive maintenance, quality control, and supply chain efficiency through AI pipelines  

It solves real-world challenges:

- âŒ Manual GPU provisioning â†’ âœ… Automated, cost-aware node groups  
- âŒ Fragile ML pipelines â†’ âœ… Reproducible, versioned deployments  
- âŒ Security gaps â†’ âœ… IAM-scoped access, IRSA, and GitHub OIDC  
- âŒ No observability â†’ âœ… Hooks for Prometheus, Grafana, and FluentBit  
- âŒ No disaster recovery â†’ âœ… Multi-region scaffolding (planned)

---

## ğŸ§± Architecture Overview

| Layer                  | Purpose                                                                 |
|------------------------|-------------------------------------------------------------------------|
| VPC + Subnets          | Isolated, AZ-resilient network for GPU workloads                        |
| IAM + Policies         | Fine-grained access for operators, CI/CD, and IRSA                      |
| EKS Cluster            | Managed Kubernetes control plane with GPU node groups                   |
| Node Groups            | Separate GPU and general-purpose pools for cost control                 |
| ALB Controller (IRSA)  | Ingress with service accountâ€“scoped permissions                         |
| Triton Inference       | GPU-enabled pods serving models via gRPC/HTTP                           |
| EFS                    | Shared model repository mounted into Triton pods                        |
| CI/CD                  | GitHub Actions + Terraform + Helm for secure automation                 |
| Observability          | Prometheus, Grafana, Loki, CloudWatch integration                       |

---

## ğŸ” Security & Compliance

- IAM roles scoped via IRSA  
- GitHub OIDC trust for CI/CD  
- Secrets managed via Kubernetes  
- Future: mTLS via Istio service mesh

---

## ğŸ“¦ Reproducibility & Lifecycle : 

- Infrastructure-as-code via Terraform  
- Helm charts for Triton + observability stack  
- Modular teardown and rebuild workflows  
- Versioned container images via Amazon ECR
- Models hosted in the aws EFS using the kubernetes persistant volume 

---

## ğŸš€ Outcomes

This isnâ€™t just a cluster â€” itâ€™s a **launchpad for scalable, a  uditable, cost-efficient AI workloads**.  
Whether you're deploying LLMs, computer vision models, or real-time inference, this platform ensures:

- ğŸ” Reproducibility  
- ğŸ” Security by default  
- ğŸ’° Cost control  
- ğŸ“ˆ Observability  
- ğŸ§© Modular onboarding  

