## ğŸš€ Enterprise-Scale MLOps: Triton Inference Ecosystem

If it isnâ€™t observable, automated, and scalable, it isnâ€™t production-ready. Most AI projects fail at the "last mile" because the infrastructure is an afterthought. This project is a full-stack implementation of a high-concurrency inference ecosystem, modeled after the architectures used by Netflix and Airbnb to serve models at scale.

### ğŸ›  The Stack
***Inference Server:*** NVIDIA Triton (Multi-framework support: PyTorch, ONNX, TensorRT)

***Orchestration:*** Kubernetes (EKS/GKE) for resilient scaling

***Infrastructure as Code:*** Terraform for reproducible GPU-enabled environments

***Automation:*** Python-driven CI/CD and model lifecycle management

***Observability:*** Prometheus & Grafana for real-time inference metrics and Loki for Logging. 



### ğŸ’¼ Business Value

This platform accelerates AI/ML adoption across industries:

- ğŸ§¬ **Biotech & Medicine** â†’ Enables faster drug discovery, medical imaging analysis, and precision diagnostics at scale  
- ğŸ’³ **Finance & Insurance** â†’ Powers fraud detection, risk modeling, and realâ€‘time customer insights with secure GPU inference  
- ğŸ¨ **Hospitality & Retail** â†’ Delivers personalized recommendations, demand forecasting, and customer experience optimization  
- ğŸŒ¾ **Agriculture & Energy** â†’ Supports crop yield prediction, resource optimization, and sustainable energy analytics  
- ğŸ­ **Manufacturing & Logistics** â†’ Improves predictive maintenance, quality control, and supply chain efficiency through AI pipelines  


<img width="800" alt="Triton-Architecture" src="https://github.com/user-attachments/assets/0cfa0d85-4ca5-41d4-9918-b46c08660d81" />



## ğŸ§± Architecture Overview

| Layer                  | Purpose                                                                 |
|------------------------|-------------------------------------------------------------------------|
| VPC + Subnets          | Isolated, AZ-resilient network for GPU workloads                        |
| IAM + Policies         | Fine-grained access for operators, CI/CD, and IRSA                      |
| EKS Cluster            | Managed Kubernetes control plane with GPU node groups                   |
| Node Groups            | Separate GPU and general-purpose pools for cost control                 |
| ALB Controller (IRSA)  | Ingress with service accountâ€“scoped permissions                         |
| Triton Inference       | GPU-enabled pods serving models via gRPC/HTTP                           |
| EFS                    | Shared model repository mounted into Triton pods                        |
| CI/CD                  | GitHub Actions + Terraform + Helm for secure automation                 |
| Observability          | Prometheus, Grafana, Loki, CloudWatch integration                       |



## ğŸ” Security & Compliance

- IAM roles scoped via IRSA  
- GitHub OIDC trust for CI/CD  
- Secrets managed via Kubernetes  
- Future: mTLS via Istio service mesh



## ğŸ“¦ Reproducibility & Lifecycle : 

- Infrastructure-as-code via Terraform  
- Helm charts for Triton + observability stack  
- Modular teardown and rebuild workflows  
- Versioned container images via Amazon ECR
- Models hosted in the aws EFS using the kubernetes persistant volume 



## ğŸš€ Key Engineering Wins

1. Optimized GPU Utilization
Implemented Dynamic Batching and Instance Groups. This allows the server to group individual inference requests together on the fly, significantly increasing throughput without manual tuning.

2. Multi-Model Concurrency
Configured the ecosystem to serve multiple models (Image Classification & NLP) simultaneously on a single GPU/Node, reducing infrastructure overhead and costsâ€”a critical requirement for Fortune 500 efficiency.

3. SRE-First Observability
Integrated custom Prometheus exporters to track:

   - Inference Latency (p95/p99)

   - Request Queue Time

   - GPU Memory Utilization

---

<img width="600" alt="Grafana-Ytiton-dashboard" src="https://github.com/user-attachments/assets/c8639bd1-4bbb-466e-a1c5-92cf450d6e6c" />



<img width="600" alt="Prometheus-Triton" src="https://github.com/user-attachments/assets/51e758eb-ccc0-4ead-97f9-9387fe3a3724" />


    
<img width="707" height="394" alt="Triton-Tesla-GPU" src="https://github.com/user-attachments/assets/a2417084-add7-4f4a-949d-49ff47979ba0" />

